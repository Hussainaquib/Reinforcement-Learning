{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d53279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b1a366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1,   100] loss: 2.259\n",
      "[1,   200] loss: 1.948\n",
      "[1,   300] loss: 1.813\n",
      "[2,   100] loss: 1.674\n",
      "[2,   200] loss: 1.620\n",
      "[2,   300] loss: 1.598\n",
      "[3,   100] loss: 1.531\n",
      "[3,   200] loss: 1.491\n",
      "[3,   300] loss: 1.480\n",
      "[4,   100] loss: 1.418\n",
      "[4,   200] loss: 1.394\n",
      "[4,   300] loss: 1.406\n",
      "[5,   100] loss: 1.336\n",
      "[5,   200] loss: 1.333\n",
      "[5,   300] loss: 1.331\n",
      "[6,   100] loss: 1.292\n",
      "[6,   200] loss: 1.266\n",
      "[6,   300] loss: 1.251\n",
      "[7,   100] loss: 1.218\n",
      "[7,   200] loss: 1.219\n",
      "[7,   300] loss: 1.185\n",
      "[8,   100] loss: 1.184\n",
      "[8,   200] loss: 1.168\n",
      "[8,   300] loss: 1.154\n",
      "[9,   100] loss: 1.134\n",
      "[9,   200] loss: 1.116\n",
      "[9,   300] loss: 1.120\n",
      "[10,   100] loss: 1.089\n",
      "[10,   200] loss: 1.072\n",
      "[10,   300] loss: 1.098\n",
      "[11,   100] loss: 1.072\n",
      "[11,   200] loss: 1.042\n",
      "[11,   300] loss: 1.031\n",
      "[12,   100] loss: 1.021\n",
      "[12,   200] loss: 1.019\n",
      "[12,   300] loss: 1.022\n",
      "[13,   100] loss: 0.991\n",
      "[13,   200] loss: 0.995\n",
      "[13,   300] loss: 0.987\n",
      "[14,   100] loss: 0.969\n",
      "[14,   200] loss: 0.951\n",
      "[14,   300] loss: 0.969\n",
      "[15,   100] loss: 0.933\n",
      "[15,   200] loss: 0.945\n",
      "[15,   300] loss: 0.951\n",
      "[16,   100] loss: 0.916\n",
      "[16,   200] loss: 0.928\n",
      "[16,   300] loss: 0.907\n",
      "[17,   100] loss: 0.899\n",
      "[17,   200] loss: 0.879\n",
      "[17,   300] loss: 0.894\n",
      "[18,   100] loss: 0.874\n",
      "[18,   200] loss: 0.882\n",
      "[18,   300] loss: 0.871\n",
      "[19,   100] loss: 0.849\n",
      "[19,   200] loss: 0.860\n",
      "[19,   300] loss: 0.875\n",
      "[20,   100] loss: 0.814\n",
      "[20,   200] loss: 0.831\n",
      "[20,   300] loss: 0.850\n",
      "[21,   100] loss: 0.810\n",
      "[21,   200] loss: 0.832\n",
      "[21,   300] loss: 0.836\n",
      "[22,   100] loss: 0.818\n",
      "[22,   200] loss: 0.806\n",
      "[22,   300] loss: 0.796\n",
      "[23,   100] loss: 0.777\n",
      "[23,   200] loss: 0.795\n",
      "[23,   300] loss: 0.798\n",
      "[24,   100] loss: 0.776\n",
      "[24,   200] loss: 0.777\n",
      "[24,   300] loss: 0.773\n",
      "[25,   100] loss: 0.745\n",
      "[25,   200] loss: 0.758\n",
      "[25,   300] loss: 0.765\n",
      "[26,   100] loss: 0.741\n",
      "[26,   200] loss: 0.740\n",
      "[26,   300] loss: 0.749\n",
      "[27,   100] loss: 0.732\n",
      "[27,   200] loss: 0.729\n",
      "[27,   300] loss: 0.748\n",
      "[28,   100] loss: 0.713\n",
      "[28,   200] loss: 0.708\n",
      "[28,   300] loss: 0.735\n",
      "[29,   100] loss: 0.714\n",
      "[29,   200] loss: 0.718\n",
      "[29,   300] loss: 0.719\n",
      "[30,   100] loss: 0.690\n",
      "[30,   200] loss: 0.700\n",
      "[30,   300] loss: 0.692\n",
      "[31,   100] loss: 0.675\n",
      "[31,   200] loss: 0.703\n",
      "[31,   300] loss: 0.690\n",
      "[32,   100] loss: 0.683\n",
      "[32,   200] loss: 0.681\n",
      "[32,   300] loss: 0.689\n",
      "[33,   100] loss: 0.676\n",
      "[33,   200] loss: 0.680\n",
      "[33,   300] loss: 0.670\n",
      "[34,   100] loss: 0.644\n",
      "[34,   200] loss: 0.660\n",
      "[34,   300] loss: 0.666\n",
      "[35,   100] loss: 0.645\n",
      "[35,   200] loss: 0.641\n",
      "[35,   300] loss: 0.635\n",
      "[36,   100] loss: 0.633\n",
      "[36,   200] loss: 0.642\n",
      "[36,   300] loss: 0.656\n",
      "[37,   100] loss: 0.626\n",
      "[37,   200] loss: 0.632\n",
      "[37,   300] loss: 0.618\n",
      "[38,   100] loss: 0.600\n",
      "[38,   200] loss: 0.623\n",
      "[38,   300] loss: 0.626\n",
      "[39,   100] loss: 0.603\n",
      "[39,   200] loss: 0.600\n",
      "[39,   300] loss: 0.600\n",
      "[40,   100] loss: 0.588\n",
      "[40,   200] loss: 0.588\n",
      "[40,   300] loss: 0.608\n",
      "[41,   100] loss: 0.590\n",
      "[41,   200] loss: 0.605\n",
      "[41,   300] loss: 0.579\n",
      "[42,   100] loss: 0.583\n",
      "[42,   200] loss: 0.575\n",
      "[42,   300] loss: 0.577\n",
      "[43,   100] loss: 0.569\n",
      "[43,   200] loss: 0.562\n",
      "[43,   300] loss: 0.571\n",
      "[44,   100] loss: 0.554\n",
      "[44,   200] loss: 0.561\n",
      "[44,   300] loss: 0.575\n",
      "[45,   100] loss: 0.544\n",
      "[45,   200] loss: 0.566\n",
      "[45,   300] loss: 0.578\n",
      "[46,   100] loss: 0.543\n",
      "[46,   200] loss: 0.538\n",
      "[46,   300] loss: 0.564\n",
      "[47,   100] loss: 0.556\n",
      "[47,   200] loss: 0.535\n",
      "[47,   300] loss: 0.554\n",
      "[48,   100] loss: 0.510\n",
      "[48,   200] loss: 0.534\n",
      "[48,   300] loss: 0.542\n",
      "[49,   100] loss: 0.499\n",
      "[49,   200] loss: 0.525\n",
      "[49,   300] loss: 0.524\n",
      "[50,   100] loss: 0.505\n",
      "[50,   200] loss: 0.527\n",
      "[50,   300] loss: 0.529\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 75 %\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load pretrained ResNet-34 model\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Define the ResNet-34 model\n",
    "model = models.resnet34(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)  # 10 classes in CIFAR-10\n",
    "model = model.cuda()  # Move the model to GPU if available\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].cuda(), data[1].cuda()  # Move data to GPU\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e8a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image\n",
    "\n",
    "# restores the tensors to their original scale\n",
    "def denorm(batch, mean=[0.1307], std=[0.3081]):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors to their original scale.\n",
    "\n",
    "    Args:\n",
    "        batch (torch.Tensor): Batch of normalized tensors.\n",
    "        mean (torch.Tensor or list): Mean used for normalization.\n",
    "        std (torch.Tensor or list): Standard deviation used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: batch of tensors without normalization applied to them.\n",
    "    \"\"\"\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).to(device)\n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).to(device)\n",
    "\n",
    "    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d804ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # If the initial prediction is wrong, don't bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect ``datagrad``\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Restore the data to its original scale\n",
    "        data_denorm = denorm(data)\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)\n",
    "\n",
    "        # Reapply normalization\n",
    "        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data_normalized)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if epsilon == 0 and len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d099fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.1\tTest Accuracy = 350 / 10000 = 0.035\n"
     ]
    }
   ],
   "source": [
    "acc, ex = test(model, device, test_loader, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f38f8c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30fe17aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "ex[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b9e912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c5740c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,\n",
       " 1,\n",
       " array([[[0.5117647 , 0.3235294 , 0.5509804 , ..., 0.7156863 ,\n",
       "          0.7196079 , 0.527451  ],\n",
       "         [0.5117647 , 0.527451  , 0.5509804 , ..., 0.7156863 ,\n",
       "          0.7156863 , 0.7196079 ],\n",
       "         [0.31568626, 0.5352941 , 0.57058823, ..., 0.527451  ,\n",
       "          0.7313726 , 0.7313726 ],\n",
       "         ...,\n",
       "         [0.        , 0.11568628, 0.10784314, ..., 0.56666666,\n",
       "          0.5862745 , 0.5862745 ],\n",
       "         [0.        , 0.15098038, 0.14313725, ..., 0.5745098 ,\n",
       "          0.57843137, 0.58235294],\n",
       "         [0.        , 0.17843138, 0.15490195, ..., 0.57843137,\n",
       "          0.39019606, 0.5862745 ]],\n",
       " \n",
       "        [[0.48823532, 0.49607846, 0.51960784, ..., 0.8686275 ,\n",
       "          0.672549  , 0.6843137 ],\n",
       "         [0.47254905, 0.48823532, 0.5117647 , ..., 0.8647059 ,\n",
       "          0.8647059 , 0.672549  ],\n",
       "         [0.66862756, 0.68823534, 0.72352946, ..., 0.8647059 ,\n",
       "          0.87254906, 0.8686275 ],\n",
       "         ...,\n",
       "         [0.16666667, 0.        , 0.        , ..., 0.5745098 ,\n",
       "          0.59411764, 0.5980392 ],\n",
       "         [0.01372549, 0.19411765, 0.17843138, ..., 0.38235292,\n",
       "          0.3862745 , 0.5901961 ],\n",
       "         [0.04509804, 0.22941178, 0.20196079, ..., 0.3862745 ,\n",
       "          0.3980392 , 0.5980392 ]],\n",
       " \n",
       "        [[0.5745098 , 0.58235294, 0.60588235, ..., 0.92745095,\n",
       "          0.7313726 , 0.7392157 ],\n",
       "         [0.5470588 , 0.5627451 , 0.5862745 , ..., 0.7       ,\n",
       "          0.7       , 0.70392156],\n",
       "         [0.5392157 , 0.55490196, 0.59411764, ..., 0.6882353 ,\n",
       "          0.69215685, 0.88823533],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.51960784,\n",
       "          0.5392157 , 0.54313725],\n",
       "         [0.        , 0.        , 0.        , ..., 0.32745096,\n",
       "          0.53137255, 0.5352941 ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.527451  ,\n",
       "          0.34313723, 0.5392157 ]]], dtype=float32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76c0e25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 2815 / 10000 = 0.2815\n",
      "Epsilon: 0.05\tTest Accuracy = 415 / 10000 = 0.0415\n",
      "Epsilon: 0.1\tTest Accuracy = 350 / 10000 = 0.035\n",
      "Epsilon: 0.15\tTest Accuracy = 367 / 10000 = 0.0367\n",
      "Epsilon: 0.2\tTest Accuracy = 366 / 10000 = 0.0366\n",
      "Epsilon: 0.25\tTest Accuracy = 373 / 10000 = 0.0373\n",
      "Epsilon: 0.3\tTest Accuracy = 370 / 10000 = 0.037\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f92500c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24e73c36",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 32, 32) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m         orig,adv,ex \u001b[38;5;241m=\u001b[39m examples[i][j]\n\u001b[1;32m     12\u001b[0m         plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m         \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py:3346\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3325\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   3326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   3327\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[0;32m-> 3346\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3350\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3352\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3355\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3357\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3361\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3365\u001b[0m     sci(__ret)\n\u001b[1;32m   3366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py:5751\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5751\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5752\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5754\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/image.py:723\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    722\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/image.py:693\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    691\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 32, 32) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt = 0\n",
    "plt.figure(figsize=(8,10))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(len(examples[i])):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(f\"Eps: {epsilons[i]}\", fontsize=14)\n",
    "        orig,adv,ex = examples[i][j]\n",
    "        plt.title(f\"{orig} -> {adv}\")\n",
    "        plt.imshow(ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57be23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
