{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be25b81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 170498071/170498071 [00:32<00:00, 5220790.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Images saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Define the transform to convert PIL images to tensors and normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Download the CIFAR-10 training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Randomly select 50000 samples from the dataset\n",
    "np.random.seed(42)  # For reproducibility\n",
    "sample_indices = np.random.choice(len(trainset), 50000, replace=False)\n",
    "sample_subset = torch.utils.data.Subset(trainset, sample_indices)\n",
    "\n",
    "# Create a directory to save the images\n",
    "os.makedirs('./cifar10_sample', exist_ok=True)\n",
    "\n",
    "# Save images to the directory\n",
    "for i, (image, label) in enumerate(sample_subset):\n",
    "    save_path = f'./cifar10_sample/{i:04d}.png'\n",
    "    save_image(image, save_path)\n",
    "\n",
    "print(\"Images saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733bc33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /home/anuj/anaconda3/lib/python3.9/site-packages (from tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/anuj/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /home/anuj/anaconda3/lib/python3.9/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/anuj/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/anuj/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/anuj/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.64.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Downloading keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/anuj/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.25.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/anuj/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Downloading optree-0.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/anuj/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/anuj/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/anuj/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/anuj/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/anuj/anaconda3/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/anuj/anaconda3/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/anuj/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/anuj/anaconda3/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/anuj/anaconda3/lib/python3.9/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/anuj/anaconda3/lib/python3.9/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/anuj/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/anuj/anaconda3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.16.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.64.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.9/311.9 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, h5py, grpcio, google-pasta, gast, astunparse, absl-py, rich, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gravity-gae 0.0.0 requires networkx==2.2, but you have networkx 3.1 which is incompatible.\n",
      "gravity-gae 0.0.0 requires tensorflow==1.*, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.64.1 h5py-3.11.0 keras-3.3.3 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c11146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 13:57:51.858651: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-13 13:57:51.900015: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 13:57:52.495700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-13 13:58:06.694914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5984 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:21:00.0, compute capability: 7.5\n",
      "2024-06-13 13:58:24.108961: W external/local_tsl/tsl/framework/bfc_allocator.cc:487] Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.16GiB (rounded to 9830400000)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-06-13 13:58:24.109021: I external/local_tsl/tsl/framework/bfc_allocator.cc:1044] BFCAllocator dump for GPU_0_bfc\n",
      "2024-06-13 13:58:24.109045: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (256): \tTotal Chunks: 21, Chunks in use: 21. 5.2KiB allocated for chunks. 5.2KiB in use in bin. 1.1KiB client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109061: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109076: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109090: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (2048): \tTotal Chunks: 3, Chunks in use: 2. 9.2KiB allocated for chunks. 7.0KiB in use in bin. 6.8KiB client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109102: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (4096): \tTotal Chunks: 1, Chunks in use: 0. 4.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109115: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109127: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109139: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109155: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (65536): \tTotal Chunks: 3, Chunks in use: 2. 208.8KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109169: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (131072): \tTotal Chunks: 2, Chunks in use: 1. 288.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109182: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 256.0KiB allocated for chunks. 256.0KiB in use in bin. 256.0KiB client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109211: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109223: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109233: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109244: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109258: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (8388608): \tTotal Chunks: 4, Chunks in use: 3. 39.47MiB allocated for chunks. 24.00MiB in use in bin. 24.00MiB client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109269: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109280: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109290: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109301: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109317: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0. 5.80GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-13 13:58:24.109330: I external/local_tsl/tsl/framework/bfc_allocator.cc:1067] Bin for 9.16GiB was 256.00MiB, Chunk State: \n",
      "2024-06-13 13:58:24.109349: I external/local_tsl/tsl/framework/bfc_allocator.cc:1073]   Size: 5.80GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 8.00MiB | Requested Size: 8.00MiB | in_use: 1 | bin_num: -1\n",
      "2024-06-13 13:58:24.109358: I external/local_tsl/tsl/framework/bfc_allocator.cc:1080] Next region of size 6274940928\n",
      "2024-06-13 13:58:24.109371: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000000 of size 256 next 1\n",
      "2024-06-13 13:58:24.109380: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000100 of size 1280 next 2\n",
      "2024-06-13 13:58:24.109390: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000600 of size 256 next 3\n",
      "2024-06-13 13:58:24.109398: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000700 of size 256 next 4\n",
      "2024-06-13 13:58:24.109407: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000800 of size 256 next 6\n",
      "2024-06-13 13:58:24.109416: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000900 of size 256 next 7\n",
      "2024-06-13 13:58:24.109425: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000a00 of size 256 next 5\n",
      "2024-06-13 13:58:24.109433: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000b00 of size 256 next 8\n",
      "2024-06-13 13:58:24.109442: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000c00 of size 256 next 13\n",
      "2024-06-13 13:58:24.109451: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000d00 of size 256 next 11\n",
      "2024-06-13 13:58:24.109460: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000e00 of size 256 next 12\n",
      "2024-06-13 13:58:24.109469: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4000f00 of size 256 next 18\n",
      "2024-06-13 13:58:24.109477: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4001000 of size 256 next 23\n",
      "2024-06-13 13:58:24.109486: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4001100 of size 256 next 16\n",
      "2024-06-13 13:58:24.109495: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4001200 of size 256 next 17\n",
      "2024-06-13 13:58:24.109503: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4001300 of size 256 next 25\n",
      "2024-06-13 13:58:24.109512: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7fc7d4001400 of size 4864 next 9\n",
      "2024-06-13 13:58:24.109522: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4002700 of size 3584 next 10\n",
      "2024-06-13 13:58:24.109532: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7fc7d4003500 of size 147456 next 15\n",
      "2024-06-13 13:58:24.109540: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4027500 of size 73728 next 14\n",
      "2024-06-13 13:58:24.109549: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4039500 of size 262144 next 21\n",
      "2024-06-13 13:58:24.109558: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d4079500 of size 73728 next 31\n",
      "2024-06-13 13:58:24.109567: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d408b500 of size 256 next 30\n",
      "2024-06-13 13:58:24.109576: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d408b600 of size 256 next 29\n",
      "2024-06-13 13:58:24.109585: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d408b700 of size 256 next 26\n",
      "2024-06-13 13:58:24.109595: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d408b800 of size 256 next 32\n",
      "2024-06-13 13:58:24.109604: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d408b900 of size 256 next 35\n",
      "2024-06-13 13:58:24.109613: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d408ba00 of size 256 next 36\n",
      "2024-06-13 13:58:24.109622: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7fc7d408bb00 of size 2304 next 33\n",
      "2024-06-13 13:58:24.109631: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d408c400 of size 3584 next 34\n",
      "2024-06-13 13:58:24.109639: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7fc7d408d200 of size 66304 next 28\n",
      "2024-06-13 13:58:24.109649: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d409d500 of size 147456 next 27\n",
      "2024-06-13 13:58:24.109660: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7fc7d40c1500 of size 16220160 next 20\n",
      "2024-06-13 13:58:24.109670: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d5039500 of size 8388608 next 19\n",
      "2024-06-13 13:58:24.109680: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d5839500 of size 8388608 next 22\n",
      "2024-06-13 13:58:24.109689: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7fc7d6039500 of size 8388608 next 24\n",
      "2024-06-13 13:58:24.109699: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7fc7d6839500 of size 6232763136 next 18446744073709551615\n",
      "2024-06-13 13:58:24.109708: I external/local_tsl/tsl/framework/bfc_allocator.cc:1105]      Summary of in-use Chunks by size: \n",
      "2024-06-13 13:58:24.109719: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 21 Chunks of size 256 totalling 5.2KiB\n",
      "2024-06-13 13:58:24.109730: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-06-13 13:58:24.109740: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 2 Chunks of size 3584 totalling 7.0KiB\n",
      "2024-06-13 13:58:24.109750: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 2 Chunks of size 73728 totalling 144.0KiB\n",
      "2024-06-13 13:58:24.109761: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 147456 totalling 144.0KiB\n",
      "2024-06-13 13:58:24.109772: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 262144 totalling 256.0KiB\n",
      "2024-06-13 13:58:24.109796: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 3 Chunks of size 8388608 totalling 24.00MiB\n",
      "2024-06-13 13:58:24.109807: I external/local_tsl/tsl/framework/bfc_allocator.cc:1112] Sum Total of in-use chunks: 24.54MiB\n",
      "2024-06-13 13:58:24.109817: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Total bytes in pool: 6274940928 memory_limit_: 6274940928 available bytes: 0 curr_region_allocation_bytes_: 12549881856\n",
      "2024-06-13 13:58:24.109832: I external/local_tsl/tsl/framework/bfc_allocator.cc:1119] Stats: \n",
      "Limit:                      6274940928\n",
      "InUse:                        25736704\n",
      "MaxInUse:                     42025216\n",
      "NumAllocs:                          79\n",
      "MaxAllocSize:                  8388608\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-06-13 13:58:24.109847: W external/local_tsl/tsl/framework/bfc_allocator.cc:499] *___________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Train the VAE model\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Extract embeddings from the encoder part\u001b[39;00m\n\u001b[1;32m     80\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mpredict(train_images)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assuming you want z_mean\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Define constants\n",
    "IMAGE_SIZE = (128, 128)\n",
    "LATENT_DIM = 32 #100\n",
    "BATCH_SIZE = 100 #32\n",
    "EPOCHS = 500\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_images(image_folder):\n",
    "    image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder)]\n",
    "    images = []\n",
    "    for img_path in image_paths:\n",
    "        img = load_img(img_path, target_size=IMAGE_SIZE)\n",
    "        img = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load and preprocess images\n",
    "train_images = load_and_preprocess_images(\"cifar10_sample\")\n",
    "\n",
    "# Define the encoder part of VAE\n",
    "encoder_inputs = layers.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "z_mean = layers.Dense(LATENT_DIM)(x)\n",
    "z_log_var = layers.Dense(LATENT_DIM)(x)\n",
    "\n",
    "# Define sampling layer\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(tf.keras.backend.shape(z_mean)[0], LATENT_DIM), mean=0., stddev=1.)\n",
    "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Define the decoder part of VAE\n",
    "decoder_inputs = layers.Input(shape=(LATENT_DIM,))\n",
    "x = layers.Dense(32*32*64, activation='relu')(decoder_inputs)\n",
    "x = layers.Reshape((32, 32, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(3, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# Define the VAE model\n",
    "encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = models.Model(decoder_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "outputs = decoder(encoder(encoder_inputs)[2])\n",
    "vae = models.Model(encoder_inputs, outputs, name='vae')\n",
    "\n",
    "# Custom VAE layer for loss calculation\n",
    "class VAELossLayer(layers.Layer):\n",
    "    def call(self, inputs, outputs, z_mean, z_log_var):\n",
    "        reconstruction_loss = tf.keras.losses.binary_crossentropy(tf.keras.backend.flatten(inputs), tf.keras.backend.flatten(outputs))\n",
    "        reconstruction_loss *= IMAGE_SIZE[0] * IMAGE_SIZE[1] * 3\n",
    "        kl_loss = 1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var)\n",
    "        kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
    "        self.add_loss(vae_loss)\n",
    "        return outputs\n",
    "\n",
    "# Integrate the custom loss layer into the model\n",
    "outputs = VAELossLayer()(encoder_inputs, outputs, z_mean, z_log_var)\n",
    "vae = models.Model(encoder_inputs, outputs, name='vae')\n",
    "\n",
    "# Compile the VAE model\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# Train the VAE model\n",
    "vae.fit(train_images, train_images, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Extract embeddings from the encoder part\n",
    "embeddings = encoder.predict(train_images)[0]  # Assuming you want z_mean\n",
    "\n",
    "# Save or use embeddings as needed\n",
    "np.save(\"cifar10.npy\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3472e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
